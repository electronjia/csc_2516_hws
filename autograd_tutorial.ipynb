{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of sin(pi) is -0.9998995297042174\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def taylor_sine(x):\n",
    "    ans = currterm = x\n",
    "    i = 0\n",
    "\n",
    "    while np.abs(currterm) > 0.001:\n",
    "        currterm = -currterm * x**2 / ((2*i+3)*(2*i+2))\n",
    "        ans = ans + currterm\n",
    "        i += 1\n",
    "\n",
    "    return ans\n",
    "\n",
    "grad_sine = grad(taylor_sine)\n",
    "print(f\"Gradient of sin(pi) is {grad_sine(np.pi)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete example of autograd application on logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.772588722239781\n",
      "Trained loss: 0.026923541797959805\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x/2.)+1)\n",
    "\n",
    "# y = sigmoid(xw)\n",
    "def logistic_predictions(weights, inputs):\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "# Cost function\n",
    "def training_loss(weights):\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "\n",
    "    # Target is 0 or 1\n",
    "    label_probabilities = preds * targets + (1-preds)*(1-targets)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "\n",
    "\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "\n",
    "targets = np.array([True, True, False, True])\n",
    "\n",
    "\n",
    "training_gradient_fun = grad(training_loss)\n",
    "\n",
    "weights = np.array([0.0,0.0,0.0])\n",
    "\n",
    "print(f\"Initial loss: {training_loss(weights)}\")\n",
    "\n",
    "# Epoch = 100 and alpha=0.01\n",
    "for i in range(100):\n",
    "    weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "\n",
    "print(f\"Trained loss: {training_loss(weights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend autograd by defining your own primitive and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: [9.09503860e-19 1.34000000e+01 6.39073344e-30]\n",
      " This module defines VJPs for\n",
      "each NumPy op. Each line is a call to defvjp which is a thin wrapper\n",
      "which just adds stuff to a Python dict which stores all the VJP functions.\n",
      "For each op, we need to specify a VJP for each of its arguments. Exercise: write VJPs for division\n",
      "and elementwise log.\n",
      "The VJP is\n",
      "represented as a function which takes in the output error signal g, the value\n",
      "of the node ans, and the arguments to the op (which, remember, are Node\n",
      "instances). The function returns the input error signal for the corresponding\n",
      "argument.\n"
     ]
    }
   ],
   "source": [
    "# Code found here: https://github.com/HIPS/autograd/blob/master/examples/define_gradient.py\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd.extend import primitive, defvjp\n",
    "\n",
    "@primitive\n",
    "def logsumexp(x):\n",
    "\n",
    "    # Numerically stable log(sum(exp(x)))\n",
    "\n",
    "    max_x = np.max(x)\n",
    "\n",
    "    return max_x + np.log(np.sum(np.exp(x-max_x)))\n",
    "\n",
    "def logsumexp_vjp(ans, x):\n",
    "    x_shape = x.shape\n",
    "\n",
    "    return lambda g: np.full(x_shape, g)* np.exp(x - np.full(x.shape, ans))\n",
    "\n",
    "defvjp(logsumexp, logsumexp_vjp)\n",
    "\n",
    "\n",
    "# Now we can use logsumexp anywhere, including inside of a larger function that we wanna differentiate\n",
    "def example_func(y):\n",
    "    z = y**2\n",
    "    lse = logsumexp(z)\n",
    "\n",
    "    return np.sum(lse)\n",
    "\n",
    "grad_of_example = grad(example_func)\n",
    "print(f\"Gradient: {grad_of_example(np.array([1.5, 6.7, 1e-10]))}\")\n",
    "\n",
    "print(\"\"\" This module defines VJPs for\n",
    "each NumPy op. Each line is a call to defvjp which is a thin wrapper\n",
    "which just adds stuff to a Python dict which stores all the VJP functions.\n",
    "For each op, we need to specify a VJP for each of its arguments. Exercise: write VJPs for division\n",
    "and elementwise log.\n",
    "The VJP is\n",
    "represented as a function which takes in the output error signal g, the value\n",
    "of the node ans, and the arguments to the op (which, remember, are Node\n",
    "instances). The function returns the input error signal for the corresponding\n",
    "argument.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
